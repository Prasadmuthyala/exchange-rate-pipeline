{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 3a954dea-b6f8-4b1b-bc24-3c77fb418ac8\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 3a954dea-b6f8-4b1b-bc24-3c77fb418ac8 to get into ready status...\nSession 3a954dea-b6f8-4b1b-bc24-3c77fb418ac8 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import boto3\n# Initialize boto3 Glue client to list tables\nclient = boto3.client('glue')\n\n# Step 1: Specify your database name\ndatabase_name = 'exchangerawdb'\n\n# Step 2: List all tables in the specified database\nresponse = client.get_tables(DatabaseName=database_name)\n\n# Step 3: Filter tables by pattern (e.g., starts with 'results')\nmatching_tables = [table['Name'] for table in response['TableList'] if table['Name'].startswith(\"results\")]\n\n# Check if we have matching tables\nif matching_tables:\n    # Assuming you want to use the first matching table\n    table_name = matching_tables[0]\n    print(f\"Selected table: {table_name}\")\nelse:\n    print(\"No matching table found.\")\n\n# Step 4: Create a dynamic frame using the selected table\ndyf = glueContext.create_dynamic_frame.from_catalog(\n    database=database_name,\n    table_name=table_name\n)\n\n# Step 5: Show the data (for inspection)\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Selected table: results_cb9cef5a93649a0062b4776bd8e0ca6e\nroot\n|-- statusCode: int\n|-- body: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------------+\n|statusCode|                body|\n+----------+--------------------+\n|       200|{\"conversion_rate...|\n+----------+--------------------+\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import from_json, col,explode\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, MapType\n\n# Define the schema for the JSON structure inside the \"body\" column\nschema = StructType([\n    StructField(\"conversion_rates\", MapType(StringType(), DoubleType())),\n    StructField(\"time_last_update_utc\", StringType()),\n    StructField(\"time_next_update_utc\", StringType()),\n    StructField(\"timestamp\", StringType())\n])\n\n# Parse the JSON string in the \"body\" column\ndf_parsed = df.withColumn(\"parsed_body\", from_json(col(\"body\"), schema))\n\n# Now you can select the parsed columns and display them\n# df_parsed.select(\"parsed_body\").show(truncate=False)\n\n# Select and extract specific fields\ndf_flattened = df_parsed.select(\n    col(\"parsed_body.conversion_rates\").alias(\"conversion_rates\"),\n    col(\"parsed_body.time_last_update_utc\").alias(\"time_last_update_utc\"),\n    col(\"parsed_body.time_next_update_utc\").alias(\"time_next_update_utc\"),\n    col(\"parsed_body.timestamp\").alias(\"timestamp\")\n)\n\n# Now let's look at the data\n# df_flattened.show(truncate=False)\n# Explode the conversion_rates map into separate rows\ndf_exploded = df_flattened.select(\n    col(\"time_last_update_utc\"),\n    col(\"time_next_update_utc\"),\n    col(\"timestamp\"),\n    explode(col(\"conversion_rates\")).alias(\"currency\", \"rate\")\n)\n\n# Now let's view the exploded data\ndf_exploded.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------------------+-------------------------------+--------------------------+--------+---------+\n|time_last_update_utc           |time_next_update_utc           |timestamp                 |currency|rate     |\n+-------------------------------+-------------------------------+--------------------------+--------+---------+\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|USD     |1.0      |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AED     |3.6725   |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AFN     |67.9608  |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|ALL     |93.2927  |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AMD     |394.4143 |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|ANG     |1.79     |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AOA     |918.8884 |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|ARS     |1011.75  |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AUD     |1.5363   |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AWG     |1.79     |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|AZN     |1.7003   |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BAM     |1.8508   |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BBD     |2.0      |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BDT     |119.5149 |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BGN     |1.8509   |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BHD     |0.376    |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BIF     |2928.5107|\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BMD     |1.0      |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BND     |1.3402   |\n|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|2024-11-30 13:44:09.305899|BOB     |6.9272   |\n+-------------------------------+-------------------------------+--------------------------+--------+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import row_number, col\nfrom pyspark.sql.window import Window\n\n# Define a window specification to order by the 'currency' column\nwindowSpec = Window.orderBy(\"currency\")\n\n# Add a sequential 'ID' column\ndf_exploded_with_id = df_exploded.withColumn(\"ID\", row_number().over(windowSpec)).select(\"ID\",\"currency\",\"rate\",\"timestamp\",\"time_last_update_utc\",\"time_next_update_utc\")\n\n# Show the result\ndf_exploded_with_id.show(truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+--------+---------+--------------------------+-------------------------------+-------------------------------+\n|ID |currency|rate     |timestamp                 |time_last_update_utc           |time_next_update_utc           |\n+---+--------+---------+--------------------------+-------------------------------+-------------------------------+\n|1  |AED     |3.6725   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|2  |AFN     |67.9608  |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|3  |ALL     |93.2927  |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|4  |AMD     |394.4143 |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|5  |ANG     |1.79     |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|6  |AOA     |918.8884 |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|7  |ARS     |1011.75  |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|8  |AUD     |1.5363   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|9  |AWG     |1.79     |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|10 |AZN     |1.7003   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|11 |BAM     |1.8508   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|12 |BBD     |2.0      |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|13 |BDT     |119.5149 |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|14 |BGN     |1.8509   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|15 |BHD     |0.376    |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|16 |BIF     |2928.5107|2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|17 |BMD     |1.0      |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|18 |BND     |1.3402   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|19 |BOB     |6.9272   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n|20 |BRL     |6.0138   |2024-11-30 13:44:09.305899|Sat, 30 Nov 2024 00:00:02 +0000|Sun, 01 Dec 2024 00:00:02 +0000|\n+---+--------+---------+--------------------------+-------------------------------+-------------------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\nfrom pyspark.sql.functions import to_timestamp, date_format,substring\n\n# Assuming the column with the date string is named 'time_last_update_utc'\ndf_converted = df_exploded_with_id.withColumn(\n    \"last_update_date\", \n    date_format(to_timestamp(col(\"time_last_update_utc\"), \"EEE, dd MMM yyyy HH:mm:ss Z\"), \"dd-MM-yyyy\")\n).withColumn(\n    \"next_update_date\", \n    date_format(to_timestamp(col(\"time_next_update_utc\"), \"EEE, dd MMM yyyy HH:mm:ss Z\"), \"dd-MM-yyyy\")\n).withColumn(\n    \"date\", \n    date_format(to_timestamp(substring(col(\"timestamp\"),0,11)), \"dd-MM-yyyy\")\n).select(\"ID\",\"currency\",\"rate\",\"date\",\"last_update_date\",\"next_update_date\")\n\n\n# Show the result\ndf_converted.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+--------+---------+----------+----------------+----------------+\n| ID|currency|     rate|      date|last_update_date|next_update_date|\n+---+--------+---------+----------+----------------+----------------+\n|  1|     AED|   3.6725|30-11-2024|      30-11-2024|      01-12-2024|\n|  2|     AFN|  67.9608|30-11-2024|      30-11-2024|      01-12-2024|\n|  3|     ALL|  93.2927|30-11-2024|      30-11-2024|      01-12-2024|\n|  4|     AMD| 394.4143|30-11-2024|      30-11-2024|      01-12-2024|\n|  5|     ANG|     1.79|30-11-2024|      30-11-2024|      01-12-2024|\n|  6|     AOA| 918.8884|30-11-2024|      30-11-2024|      01-12-2024|\n|  7|     ARS|  1011.75|30-11-2024|      30-11-2024|      01-12-2024|\n|  8|     AUD|   1.5363|30-11-2024|      30-11-2024|      01-12-2024|\n|  9|     AWG|     1.79|30-11-2024|      30-11-2024|      01-12-2024|\n| 10|     AZN|   1.7003|30-11-2024|      30-11-2024|      01-12-2024|\n| 11|     BAM|   1.8508|30-11-2024|      30-11-2024|      01-12-2024|\n| 12|     BBD|      2.0|30-11-2024|      30-11-2024|      01-12-2024|\n| 13|     BDT| 119.5149|30-11-2024|      30-11-2024|      01-12-2024|\n| 14|     BGN|   1.8509|30-11-2024|      30-11-2024|      01-12-2024|\n| 15|     BHD|    0.376|30-11-2024|      30-11-2024|      01-12-2024|\n| 16|     BIF|2928.5107|30-11-2024|      30-11-2024|      01-12-2024|\n| 17|     BMD|      1.0|30-11-2024|      30-11-2024|      01-12-2024|\n| 18|     BND|   1.3402|30-11-2024|      30-11-2024|      01-12-2024|\n| 19|     BOB|   6.9272|30-11-2024|      30-11-2024|      01-12-2024|\n| 20|     BRL|   6.0138|30-11-2024|      30-11-2024|      01-12-2024|\n+---+--------+---------+----------+----------------+----------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_converted.write.format(\"parquet\").mode(\"overwrite\").save(\"s3://exchangerateapi/intermediateTransformedData/\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# s3output = glueContext.getSink(\n#   path=\"s3://bucket_name/folder_name\",\n#   connection_type=\"s3\",\n#   updateBehavior=\"UPDATE_IN_DATABASE\",\n#   partitionKeys=[],\n#   compression=\"snappy\",\n#   enableUpdateCatalog=True,\n#   transformation_ctx=\"s3output\",\n# )\n# s3output.setCatalogInfo(\n#   catalogDatabase=\"demo\", catalogTableName=\"populations\"\n# )\n# s3output.setFormat(\"glueparquet\")\n# s3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		}
	]
}